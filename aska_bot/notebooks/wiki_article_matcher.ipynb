{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Article Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithm:\n",
    "1. regex replacement operations\n",
    "2. word tokenize\n",
    "3. stanford NER and nltk pos\n",
    "4. Determine pronoun chunking technique\n",
    "5. if noun/pronoun ambiguous (first result does not match exactly, ask to choose from list)\n",
    "6. use new nouns/pronouns to find article\n",
    "\n",
    "https://www.mediawiki.org/wiki/API:Search#GET_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing:\n",
    "- test query preprocessing\n",
    "- test noun/pronoun/query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core modules\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import requests\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "# nltk modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# set environment var with path to stanford-ner.jar\n",
    "os.environ[\"CLASSPATH\"] = \"../models/stanford-ner.jar\"\n",
    "\n",
    "# instantiate stanford ner\n",
    "stanford_ner = StanfordNERTagger(\"../models/english.conll.4class.distsim.crf.ser.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preprocess Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where is Stephen Curry cell phone Draymond Green World Health Organization?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    # remove possessive\n",
    "    query = query.replace(\"\\'s\", \"\")\n",
    "    query = query.replace(\"s\\'\", \"\")\n",
    "\n",
    "    # remove punctuation\n",
    "    query = query.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # remove stopwords\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    for word in eng_stopwords:\n",
    "        if re.search(r\" {} \".format(word), query):\n",
    "            query = query.replace(r\"{} \".format(word), \"\")\n",
    "\n",
    "    # word tokenize query\n",
    "    query_lst = [word for word in word_tokenize(query)]\n",
    "    \n",
    "    return query_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where',\n",
       " 'Stephen',\n",
       " 'Curry',\n",
       " 'cell',\n",
       " 'phone',\n",
       " 'Draymond',\n",
       " 'Green',\n",
       " 'World',\n",
       " 'Health',\n",
       " 'Organization']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Wikipedia API Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaAPI:\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_titles_by_text(query, limit = 10):\n",
    "        params = {\n",
    "            \"srsearch\": query,\n",
    "            \"srlimit\": limit,\n",
    "            \"srwhat\" : \"text\",\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "        }\n",
    "        res = requests.get(WikipediaAPI.url, params)\n",
    "        return res.json()\n",
    "\n",
    "    @staticmethod\n",
    "    def has_near_match(query):\n",
    "        params = {\n",
    "            \"srsearch\": query,\n",
    "            \"srwhat\" : \"nearmatch\",\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "        }\n",
    "        res = requests.get(WikipediaAPI.url, params)\n",
    "        return len(res.json()[\"query\"][\"search\"]) > 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_titles_by_query(query, results = 10):\n",
    "        return wikipedia.search(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consecutive_ner(nouns):\n",
    "    res = []\n",
    "    temp = []\n",
    "    is_entity = False\n",
    "    ner_tags = stanford_ner.tag(nouns)\n",
    "    for word, tag in ner_tags:\n",
    "        if (tag != \"O\" and is_entity) or (tag == \"O\" and not is_entity):\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            if temp:\n",
    "                res.append(temp)\n",
    "            temp = [word]\n",
    "            is_entity = not is_entity\n",
    "        \n",
    "    if temp:\n",
    "        res.append(temp)\n",
    "        \n",
    "    return res\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_combinations(lst):\n",
    "    all_combinations = []\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i + 1, len(lst)):\n",
    "            all_combinations.append(lst[i:j + 1])\n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_nouns(nouns):\n",
    "    return chunk_nouns_recursive(nouns, list())\n",
    "\n",
    "def chunk_nouns_recursive(rem_nouns, chunks):\n",
    "    if len(rem_nouns) == 0:\n",
    "        return chunks\n",
    "\n",
    "    if len(rem_nouns) == 1:\n",
    "        chunks.append(rem_nouns[0])\n",
    "        return chunks\n",
    "\n",
    "    all_combinations = get_all_combinations(rem_nouns)\n",
    "\n",
    "    results = []\n",
    "    for combination in all_combinations:\n",
    "        combination_str = \" \".join(combination)\n",
    "        new_rem_nouns = rem_nouns.copy()\n",
    "        new_chunks = chunks.copy()\n",
    "        if has_near_match(combination_str):\n",
    "            for noun in combination:\n",
    "                new_rem_nouns.remove(noun)\n",
    "            new_chunks.append(combination_str)\n",
    "            result = chunk_nouns_recursive(new_rem_nouns, new_chunks)\n",
    "            results.append(result)\n",
    "\n",
    "    if results:\n",
    "        max_result = min(results, key=len)\n",
    "        return max_result\n",
    "    else:\n",
    "        chunks.extend(rem_nouns)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(query_lst):\n",
    "    res = []\n",
    "    for consecutive_ner in get_consecutive_ner(query_lst):\n",
    "        if len(consecutive_ner) > 1:\n",
    "            res.extend(chunk_nouns(consecutive_ner))\n",
    "        else:\n",
    "            res.append(consecutive_ner[0])\n",
    "    tagged = nltk.pos_tag(res)\n",
    "    res = [tag[0] for tag in tagged if tag[1][0] == \"N\"]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Get Relevant Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavent_searches(query_lst, chunks, n_searches):\n",
    "    assert(n_searches > len(chunks) + 1)\n",
    "    relavent_searches = []\n",
    "    n_searches_per_chunk = n_searches // (len(chunks) + 1)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        n_chunk_searches = 0\n",
    "        searches = WikipediaAPI.find_titles_by_query(chunk, n_searches)\n",
    "        for search in searches:\n",
    "            # find searches not already found\n",
    "            if n_chunk_searches >= n_searches_per_chunk:\n",
    "                break\n",
    "            if search not in relavent_searches:\n",
    "                relavent_searches.append(search)\n",
    "                n_chunk_searches += 1\n",
    "            \n",
    "    searches = WikipediaAPI.find_titles_by_query(\" \".join(query_lst), n_searches)\n",
    "    for search in searches:\n",
    "        if len(relavent_searches) >= n_searches:\n",
    "            break\n",
    "        if search not in relavent_searches:\n",
    "            relavent_searches.append(search)\n",
    "    \n",
    "    return relavent_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/page_dataset/page_dataset_10.csv\"\n",
    "page_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(page_df, n_searches = 10):\n",
    "    acc = 0\n",
    "    for index, row in page_df.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print(index)\n",
    "        question = row[\"question\"]\n",
    "        label = row[\"label\"]\n",
    "        query_lst = preprocess_query(question)\n",
    "        chunks = get_chunks(query_lst)\n",
    "        relevant_searches = get_relavent_searches(query_lst, chunks, n_searches)\n",
    "        if label in relevant_searches:\n",
    "            acc += 1\n",
    "    return acc / len(page_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "acc = get_acc(page_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
