{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Article Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithm:\n",
    "1. regex replacement operations\n",
    "2. word tokenize\n",
    "3. stanford NER and nltk pos\n",
    "4. Determine pronoun chunking technique\n",
    "5. if noun/pronoun ambiguous (first result does not match exactly, ask to choose from list)\n",
    "6. use new nouns/pronouns to find article\n",
    "\n",
    "https://www.mediawiki.org/wiki/API:Search#GET_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing:\n",
    "- test query preprocessing\n",
    "- test noun/pronoun/query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core modules\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import requests\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "# nltk modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# set environment var with path to stanford-ner.jar\n",
    "os.environ[\"CLASSPATH\"] = \"../models/stanford-ner.jar\"\n",
    "\n",
    "# instantiate stanford ner\n",
    "stanford_ner = StanfordNERTagger(\"../models/english.conll.4class.distsim.crf.ser.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preprocess Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What religions and idea of thought is heresy cited as being used frequently in?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, remove_possessive = True, \n",
    "                  remove_punctuation = True, \n",
    "                  remove_stopwords = True):\n",
    "    '''\n",
    "    Peforms string processing such as removing possessive, punctuation, and stopword.\n",
    "    '''\n",
    "    if remove_possessive:\n",
    "        # remove possessive\n",
    "        query = query.replace(\"\\'s\", \"\")\n",
    "        query = query.replace(\"s\\'\", \"\")\n",
    "\n",
    "    if remove_punctuation:\n",
    "        # remove punctuation\n",
    "        query = query.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    if remove_stopwords:\n",
    "        # remove stopwords\n",
    "        eng_stopwords = stopwords.words('english')\n",
    "        for word in eng_stopwords:\n",
    "            if re.search(r\" {} \".format(word), query):\n",
    "                query = query.replace(r\"{} \".format(word), \"\")\n",
    "\n",
    "    # word tokenize query\n",
    "    query_lst = [word for word in word_tokenize(query)]\n",
    "    \n",
    "    return query_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_lst = preprocess_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Wikipedia API Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaAPI:\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_titles_by_text(query, limit = 10):\n",
    "        params = {\n",
    "            \"srsearch\": query,\n",
    "            \"srlimit\": limit,\n",
    "            \"srwhat\" : \"text\",\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "        }\n",
    "        res = requests.get(WikipediaAPI.url, params)\n",
    "        return res.json()\n",
    "\n",
    "    @staticmethod\n",
    "    def has_near_match(query):\n",
    "        params = {\n",
    "            \"srsearch\": query,\n",
    "            \"srwhat\" : \"nearmatch\",\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "        }\n",
    "        res = requests.get(WikipediaAPI.url, params)\n",
    "        return len(res.json()[\"query\"][\"search\"]) > 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_titles_by_query(query, results = 10):\n",
    "        return wikipedia.search(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consec_ner(lst):\n",
    "    '''\n",
    "    Groups nouns based on consecutive named-entities and consecutive non-named-entities.\n",
    "    '''\n",
    "    consec_ner = []\n",
    "    temp = []\n",
    "    is_entity = False\n",
    "    ner_tags = stanford_ner.tag(lst)\n",
    "    \n",
    "    for word, tag in ner_tags:\n",
    "        if (tag != \"O\" and is_entity) or (tag == \"O\" and not is_entity):\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            if temp:\n",
    "                consec_ner.append(temp)\n",
    "            temp = [word]\n",
    "            is_entity = not is_entity\n",
    "    \n",
    "    # add remaning words in temp\n",
    "    if temp:\n",
    "        consec_ner.append(temp)\n",
    "        \n",
    "    return consec_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consec_combinations(lst):\n",
    "    '''\n",
    "    Gets consecutive combinations in a list where each combination has two or more elements.\n",
    "    '''\n",
    "    consec_combinations = []\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i + 1, len(lst)):\n",
    "            consec_combinations.append(lst[i:j + 1])\n",
    "            \n",
    "    return consec_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_nouns(nouns):\n",
    "    return chunk_nouns_recursive(nouns, list())\n",
    "\n",
    "def chunk_nouns_recursive(rem_nouns, chunks):\n",
    "    if len(rem_nouns) == 0:\n",
    "        return chunks\n",
    "\n",
    "    if len(rem_nouns) == 1:\n",
    "        chunks.append(rem_nouns[0])\n",
    "        return chunks\n",
    "\n",
    "    all_combinations = get_consec_combinations(rem_nouns)\n",
    "\n",
    "    results = []\n",
    "    for combination in all_combinations:\n",
    "        combination_str = \" \".join(combination)\n",
    "        new_rem_nouns = rem_nouns.copy()\n",
    "        new_chunks = chunks.copy()\n",
    "        if WikipediaAPI.has_near_match(combination_str):\n",
    "            for noun in combination:\n",
    "                new_rem_nouns.remove(noun)\n",
    "            new_chunks.append(combination_str)\n",
    "            result = chunk_nouns_recursive(new_rem_nouns, new_chunks)\n",
    "            results.append(result)\n",
    "\n",
    "    if results:\n",
    "        max_result = min(results, key=len)\n",
    "        return max_result\n",
    "    else:\n",
    "        chunks.extend(rem_nouns)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(query_lst):\n",
    "    res = []\n",
    "    for consecutive_ner in get_consec_ner(query_lst):\n",
    "        if len(consecutive_ner) > 1:\n",
    "            res.extend(chunk_nouns(consecutive_ner))\n",
    "        else:\n",
    "            res.append(consecutive_ner[0])\n",
    "    tagged = nltk.pos_tag(res)\n",
    "    res = [tag[0] for tag in tagged if tag[1][0] == \"N\"]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Get Relevant Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavent_searches(query_lst, chunks, n_searches):\n",
    "    if n_searches > len(chunks) + 1:\n",
    "        relavent_searches = []\n",
    "        n_searches_per_chunk = n_searches // (len(chunks) + 1)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            n_chunk_searches = 0\n",
    "            searches = WikipediaAPI.find_titles_by_query(chunk, n_searches)\n",
    "            for search in searches:\n",
    "                # find searches not already found\n",
    "                if n_chunk_searches >= n_searches_per_chunk:\n",
    "                    break\n",
    "                if search not in relavent_searches:\n",
    "                    relavent_searches.append(search)\n",
    "                    n_chunk_searches += 1\n",
    "\n",
    "        searches = WikipediaAPI.find_titles_by_query(\" \".join(query_lst), n_searches)\n",
    "        for search in searches:\n",
    "            if len(relavent_searches) >= n_searches:\n",
    "                break\n",
    "            if search not in relavent_searches:\n",
    "                relavent_searches.append(search)\n",
    "    else:\n",
    "        relavent_searches = WikipediaAPI.find_titles_by_query(\" \".join(query_lst), n_searches)\n",
    "    \n",
    "    return relavent_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/page_dataset/page_dataset_1.csv\"\n",
    "page_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(page_df, n_searches = 10):\n",
    "    acc = 0\n",
    "    for index, row in page_df.iterrows():\n",
    "        if index % 25 == 0:\n",
    "            print(\"Evaluating: [{}]/[{}]\".format(index, len(page_df)))\n",
    "        question = row[\"question\"]\n",
    "        label = row[\"label\"]\n",
    "        query_lst = preprocess_query(question)\n",
    "        chunks = get_chunks(query_lst)\n",
    "        relevant_searches = get_relavent_searches(query_lst, chunks, n_searches)\n",
    "        if label in relevant_searches:\n",
    "            acc += 1\n",
    "        else:\n",
    "            print(\"Question: {}\".format(question))\n",
    "            print(\"Label: {}\".format(label))\n",
    "            print(\"Chunks: {}\".format(chunks))\n",
    "            print(\"Searches: {}\".format(relevant_searches))\n",
    "    return acc / len(page_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: [0]/[440]\n",
      "Question: Who is the main character of the story?\n",
      "Label: The Legend of Zelda: Twilight Princess\n",
      "Chunks: ['main character', 'story']\n",
      "Searches: ['Protagonist', 'Oggy and the Cockroaches', 'Main Page', 'Story', 'Story within a story', 'American Horror Story', 'Bo Peep (Toy Story)', 'List of breakout characters', 'Forky', 'Character arc']\n",
      "Question: On what fault did the earthquake occur?\n",
      "Label: 2008 Sichuan earthquake\n",
      "Chunks: ['fault', 'earthquake']\n",
      "Searches: ['Fault', 'The Fault in Our Stars (film)', 'The Fault in Our Stars', 'Earthquake', '2004 Indian Ocean earthquake and tsunami', 'Lists of earthquakes', 'San Andreas Fault', 'Wasatch Fault', 'Fault (geology)', 'Alpine Fault']\n",
      "Question: What city in the United States has the highest population?\n",
      "Label: New York (state)\n",
      "Chunks: ['city', 'population']\n",
      "Searches: ['The City & the City', 'City', 'New York City', 'Population', 'World population', 'Population density', 'List of United States cities by population', 'List of United States cities by crime rate', 'List of United States cities by population density', 'United States']\n",
      "Question: In what fields of science is the genome studied?\n",
      "Label: Genetic code\n",
      "Chunks: ['fields']\n",
      "Searches: ['Fields', 'Field', 'FIELDS', 'Magnetic field', 'In Flanders Fields', 'Genome', 'Human genome', 'Science', 'Virus', 'Biology']\n",
      "Question: What kind of school does not base its admissions on academic merit?\n",
      "Label: Comprehensive school\n",
      "Chunks: ['kind', 'school', 'base', 'admissions', 'merit']\n",
      "Searches: ['Kind', 'School', 'Base', 'Admission', 'Merit', \"Who's Who Among American High School Students\", 'Student financial aid (United States)', 'College admissions in the United States', 'University of California', '2019 college admissions bribery scandal']\n",
      "Question: In what century did the term polytechnic first show up?\n",
      "Label: List of institutions using the term \"institute of technology\" or \"polytechnic\"\n",
      "Chunks: ['show up', 'century', 'term', 'polytechnic']\n",
      "Searches: ['Police lineup', 'The Show Where Sam Shows Up', 'Century', '20th Century Studios', 'Term', 'Academic term', 'Polytechnic', 'Virginia Tech', 'University of Westminster', 'Worcester Polytechnic Institute']\n",
      "Question: When was the program necessary to crawl and archive the web created?\n",
      "Label: Wayback Machine\n",
      "Chunks: ['program', 'crawl', 'web']\n",
      "Searches: ['Program', 'Computer program', 'Crawl', 'Crawl (2019 film)', 'Web', 'Dark web', 'Web search engine', 'Internet Archive', 'Robots exclusion standard', 'Aaron Swartz']\n",
      "Question: What type of relationship do herbivores have with the bacteria in their intestines?\n",
      "Label: Symbiosis\n",
      "Chunks: ['type', 'relationship', 'bacteria', 'intestines']\n",
      "Searches: ['Type', 'Blood type', 'Relationship', 'Customer relationship management', 'Bacteria', 'Gram-positive bacteria', 'Large intestine', 'Gastrointestinal tract', 'Ruminant', 'Vitamin B12']\n",
      "Question: How many sub components are there?\n",
      "Label: Canadian Armed Forces\n",
      "Chunks: ['How many', 'components']\n",
      "Searches: ['How Many', 'How Many Ways', 'How Many More Times', 'Component', 'Corsair Components', 'Symmetrical components', 'Sub-Saharan Africa', 'Received Pronunciation', 'Cell (biology)', 'Dosage form']\n",
      "Question: In 1059, who was responsible for electing the pope? \n",
      "Label: Cardinal (Catholic Church)\n",
      "Chunks: ['pope']\n",
      "Searches: ['Pope', 'Pope Francis', 'Pope John Paul II', 'Nick Pope (footballer)', 'List of popes', 'Papal conclave', 'Counts of Tusculum', 'Electoral college', 'Pope Benedict XI', 'Pope Benedict XIV']\n",
      "Evaluating: [25]/[440]\n",
      "Question: What is another term meaning Iranian languages?\n",
      "Label: Indo-Aryan languages\n",
      "Chunks: ['term', 'languages']\n",
      "Searches: ['Term', 'Academic term', 'Cracker (term)', 'Language', 'Romance languages', 'Indo-European languages', 'Iranian languages', 'Persian language', 'Meaning of life', 'Iranian peoples']\n",
      "Question: What is used a main source of light for a building during the day?\n",
      "Label: Lighting\n",
      "Chunks: ['main source', 'building', 'day']\n",
      "Searches: ['Main Source', 'Main Source (album)', 'Building', 'Listed building', 'Day', 'Day (disambiguation)', 'Pollution', 'Grow light', 'Light pollution', 'Street light']\n",
      "Question: What was one of the first times the Supreme Court tried a case regarding nondelegation?\n",
      "Label: Separation of powers under the United States Constitution\n",
      "Chunks: ['times', 'Supreme Court', 'case', 'nondelegation']\n",
      "Searches: ['The Times', 'The New York Times', 'Supreme Court of the United States', 'Supreme court', 'Case', 'Letter case', 'Nondelegation doctrine', 'Clinton v. City of New York', 'Article One of the United States Constitution', 'Constitution of the United States']\n",
      "Question: What is the acronym for British Broadcasting Corporation?\n",
      "Label: BBC Television\n",
      "Chunks: ['British Broadcasting Corporation']\n",
      "Searches: ['BBC', 'British Broadcasting Company', 'BBC World Service', 'Canadian Broadcasting Corporation', 'Munhwa Broadcasting Corporation', 'Acronym', 'British Columbia', 'Glazer ownership of Manchester United', 'Fuck', 'Abbreviation']\n",
      "Question: What religions and idea of thought is heresy cited as being used frequently in?\n",
      "Label: Issues in anarchism\n",
      "Chunks: ['idea']\n",
      "Searches: ['Idea', 'Idea (disambiguation)', 'Vodafone Idea', 'IntelliJ IDEA', 'Idea Factory', 'Gnosticism', 'Dissent', 'Heresy in Christianity', 'List of heresies in the Catholic Church', 'Protestantism']\n",
      "Question: What is materialism?\n",
      "Label: Monism\n",
      "Chunks: ['materialism']\n",
      "Searches: ['Materialism', 'Historical materialism', 'Dialectical materialism', 'Eliminative materialism', 'Economic materialism', 'Cultural materialism (anthropology)', 'Cartesian materialism', 'Cultural materialism (cultural studies)', 'Dialectical and Historical Materialism', 'Christian materialism']\n",
      "Question: While many, the perceptions of Christianity can sometimes what?\n",
      "Label: Christians\n",
      "Chunks: ['perceptions', 'Christianity']\n",
      "Searches: ['Corruption Perceptions Index', 'Perception', 'Extrasensory perception', 'Christianity', 'History of Christianity', 'Christian denomination', 'Devil', 'Persecution of Christians', 'Abrahamic religions', 'Evangelicalism']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-43a66320f9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-be71cf819f1f>\u001b[0m in \u001b[0;36mget_acc\u001b[0;34m(page_df, n_searches)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mquery_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mrelevant_searches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relavent_searches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_searches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_searches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d29e457dfa35>\u001b[0m in \u001b[0;36mget_chunks\u001b[0;34m(query_lst)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mconsecutive_ner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_consec_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsecutive_ner\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsecutive_ner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-14cd324f8c77>\u001b[0m in \u001b[0;36mget_consec_ner\u001b[0;34m(nouns_lst)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mis_entity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mner_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanford_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mner_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         stanpos_output, _stderr = java(\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stanford_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1679\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = get_acc(page_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5522727272727272"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
